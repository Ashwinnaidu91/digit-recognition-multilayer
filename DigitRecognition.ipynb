{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "#data loader\n",
    "   \n",
    "def onehotencoder(labels,start,stop):\n",
    "    encode = np.zeros((labels.shape[0],1 - start + stop))\n",
    "    for i in range(labels.shape[0]):\n",
    "        encode[i,int(labels[i])] = 1.0\n",
    "    return encode\n",
    "\n",
    "def csv2labeldata(stringname):\n",
    "    with open(stringname, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\n",
    "        result = np.array(list(reader)).astype(\"float\")\n",
    "        labels,data=np.hsplit(result, [1])\n",
    "        return labels,data \n",
    "test_labels,test_data = csv2labeldata(\"C:/BelgiumTSC/MNIST/MNIST/mnist_dataset/sample_test.csv\")\n",
    "test_labels=onehotencoder(test_labels,0,9)\n",
    "train_labels,train_data = csv2labeldata(\"C:/BelgiumTSC/MNIST/MNIST/mnist_dataset/sample_train.csv\")\n",
    "train_labels=onehotencoder(train_labels,0,9)  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X_train_img=np.reshape(train_data,(train_data.shape[0],28,28,1))\n",
    "plt.imshow(X_train_img[0,:,:,0],cmap = 'gray')\n",
    "plt.show()\n",
    "\n",
    "#definite randomize init\n",
    "\n",
    "#size of neural network\n",
    "hidden_layer_num = 3\n",
    "#train_size = 12080\n",
    "train_size = 7000\n",
    "\n",
    "test_size = 2019\n",
    "\n",
    "input_nodes = train_data.shape[1]\n",
    "h1_nodes = 400\n",
    "h2_nodes = 200\n",
    "h3_nodes = 100\n",
    "output_nodes = 10\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.00000001\n",
    "#learning_rate = 0.0001\n",
    "batch = 30\n",
    "\n",
    "#activation function\n",
    "def relu(v):\n",
    "    relu=v\n",
    "    for i in range(v.shape[0]):\n",
    "        relu[i]=np.maximum(0,v[i])\n",
    "    return relu\n",
    "\n",
    "def relu_prime(v):\n",
    "    relu=v\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(v.shape[1]):\n",
    "            if(v[i][j]>0.0):\n",
    "                relu[i][j]= 1.0\n",
    "            else:\n",
    "                relu[i][j]= 0.0\n",
    "    return relu\n",
    "    \n",
    "def softmax(x):\n",
    "    prob = np.exp(x-np.max(x))\n",
    "    p = prob/prob.sum(axis=1)\n",
    "    return p\n",
    "\n",
    "np.random.seed(12)\n",
    "#hidden layer 1\n",
    "hl1_weights = np.random.randn(input_nodes, h1_nodes)\n",
    "hl1_weights = hl1_weights/(np.amax(hl1_weights))\n",
    "hl1_biases = np.zeros((1,h1_nodes))\n",
    "\n",
    "#hidden layer 2\n",
    "hl2_weights = np.random.randn(h1_nodes ,h2_nodes)\n",
    "hl2_weights = hl2_weights/(np.amax(hl2_weights))\n",
    "hl2_biases = np.zeros((1,h2_nodes))\n",
    "\n",
    "#hidden layer 3\n",
    "hl3_weights = np.random.randn(h2_nodes ,h3_nodes)\n",
    "hl3_weights = hl3_weights/(np.amax(hl3_weights))\n",
    "hl3_biases = np.zeros((1,h3_nodes))\n",
    "\n",
    "#output layer\n",
    "ol_weights = np.random.randn(h3_nodes ,output_nodes)\n",
    "ol_weights = ol_weights/(np.amax(ol_weights))\n",
    "ol_biases = np.zeros((1,output_nodes))\n",
    "\n",
    "#actual model param tuple\n",
    "weights_and_biases = (hl1_weights,hl1_biases, hl2_weights,hl2_biases, hl3_weights,hl3_biases, ol_weights,ol_biases)\n",
    "\n",
    "def feedforward(input_data,weights_and_biases):\n",
    "    hl1_weights,hl1_biases, hl2_weights,hl2_biases, hl3_weights,hl3_biases, ol_weights,ol_biases = weights_and_biases\n",
    "\n",
    "    z1 = relu(np.dot(input_data ,hl1_weights) + hl1_biases)\n",
    "    z2 = relu(np.dot(z1 ,hl2_weights) + hl2_biases)\n",
    "    z3 = relu(np.dot(z2, hl3_weights) + hl3_biases)\n",
    "    \n",
    "    return (softmax(np.dot(z3 ,ol_weights) + ol_biases),z1,z2,z3)\n",
    "    #return (relu(np.dot(z3, ol_weights) + ol_biases),z1,z2,z3)\n",
    "def backpropogate(input_data,actual_label,probabilistic_value ,weights_and_biases, alpha):\n",
    "    pred,z1,z2,z3 = probabilistic_value\n",
    "    hl1_weights,hl1_biases, hl2_weights,hl2_biases, hl3_weights,hl3_biases, ol_weights,ol_biases = weights_and_biases\n",
    "    \n",
    "    delta4 = (pred - actual_label)  #predict - label\n",
    "    dow = np.dot(z3.T,delta4)\n",
    "    dob = np.sum(delta4)\n",
    "    delta3 = np.multiply(np.dot(delta4,ol_weights.T), relu_prime(z3))\n",
    "    \n",
    "    dhlw_3 = np.dot(z2.T,delta3) \n",
    "    dhlb_3 = np.sum(delta3)\n",
    "    \n",
    "    delta2 = np.multiply(np.dot(delta3,hl3_weights.T) , relu_prime(z2))\n",
    "    \n",
    "    dhlw_2 = np.dot(z1.T,delta2) \n",
    "    dhlb_2 = np.sum(delta2)\n",
    "\n",
    "    delta1 = np.multiply(np.dot(delta2,hl2_weights.T) , relu_prime(z1))\n",
    "    \n",
    "    dhlw_1 = np.dot(input_data.T,delta1) \n",
    "    dhlb_1 = np.sum(delta1)\n",
    "\n",
    "    #regularization and\n",
    "    #updating\n",
    "    dhlw_1 += 0.01 * hl1_weights\n",
    "    hl1_weights -= alpha*dhlw_1\n",
    "    hl1_biases -= alpha*dhlb_1\n",
    "    \n",
    "    dhlw_2 += 0.01 * hl2_weights\n",
    "    hl2_weights -= alpha*dhlw_2\n",
    "    hl2_biases -= alpha*dhlb_2\n",
    "    \n",
    "    dhlw_3 += 0.01 * hl3_weights\n",
    "    hl3_weights -= alpha*dhlw_3\n",
    "    hl3_biases -= alpha*dhlb_3\n",
    "    \n",
    "    dow += 0.01 * ol_weights\n",
    "    ol_weights -= alpha*dow\n",
    "    ol_biases -= alpha*dob\n",
    "    \n",
    "    return (hl1_weights,hl1_biases, hl2_weights,hl2_biases, hl3_weights,hl3_biases, ol_weights,ol_biases)\n",
    "\n",
    "\n",
    "def backpropogate_adam(adam_parameters, input_data,actual_label,probabilistic_value ,weights_and_biases, alpha, decay_rate_1 = None, \n",
    "              decay_rate_2 = None, epsilon = None):\n",
    "    pred,z1,z2,z3 = probabilistic_value\n",
    "    hl1_weights,hl1_biases, hl2_weights,hl2_biases, hl3_weights,hl3_biases, ol_weights,ol_biases = weights_and_biases\n",
    "    dow_m, dow_v,dob_m, dob_v, dhl3_m, dhl3_v, dhlb3_m, dhlb3_v, dhl2_m, dhl2_v, dhlb2_m, dhlb2_v, dhl1_m, dhl1_v, dhlb1_m, dhlb1_v, t = adam_parameters\n",
    "    \n",
    "    delta4 = (pred - actual_label)  #predict - label\n",
    "    dow = np.dot(z3.T,delta4)\n",
    "    dob = np.sum(delta4)\n",
    "\n",
    "    delta3 = np.multiply(np.dot(delta4,ol_weights.T), relu_prime(z3))\n",
    "    \n",
    "    dhlw_3 = np.dot(z2.T,delta3) \n",
    "    dhlb_3 = np.sum(delta3)\n",
    "    \n",
    "    delta2 = np.multiply(np.dot(delta3,hl3_weights.T) , relu_prime(z2))\n",
    "    \n",
    "    dhlw_2 = np.dot(z1.T,delta2) \n",
    "    dhlb_2 = np.sum(delta2)\n",
    "\n",
    "    delta1 = np.multiply(np.dot(delta2,hl2_weights.T) , relu_prime(z1))\n",
    "    \n",
    "    dhlw_1 = np.dot(input_data.T,delta1) \n",
    "    dhlb_1 = np.sum(delta1)\n",
    "\n",
    "    #regularization and\n",
    "    #updating\n",
    "    dhlw_1 += 0.01 * hl1_weights  \n",
    "    dhlw_2 += 0.01 * hl2_weights    \n",
    "    dhlw_3 += 0.01 * hl3_weights\n",
    "    dow += 0.01 * ol_weights\n",
    "          \n",
    "    t =t + 1 # Increment Time Step\n",
    "            \n",
    "    # Computing 1st and 2nd moment for each layer\n",
    "    dow_m = dow_m * decay_rate_1 + (1- decay_rate_1) * dow\n",
    "    dob_m = dob_m * decay_rate_1 + (1- decay_rate_1) * dob\n",
    "    \n",
    "    dhl3_m = dhl3_m * decay_rate_1 + (1- decay_rate_1) * dhlw_3\n",
    "    dhlb3_m = dhlb3_m * decay_rate_1 + (1- decay_rate_1) * dhlb_3\n",
    "    \n",
    "    dhl2_m = dhl2_m * decay_rate_1 + (1- decay_rate_1) * dhlw_2\n",
    "    dhlb2_m = dhlb2_m * decay_rate_1 + (1- decay_rate_1) * dhlb_2\n",
    "    \n",
    "    dhl1_m = dhl1_m * decay_rate_1 + (1- decay_rate_1) * dhlw_1\n",
    "    dhlb1_m = dhlb1_m * decay_rate_1 + (1- decay_rate_1) * dhlb_1\n",
    "    \n",
    "    \n",
    "    dow_v = dow_v * decay_rate_2 + (1- decay_rate_2) * (dow ** 2)\n",
    "    dob_v = dob_v * decay_rate_2 + (1- decay_rate_2) * (dob ** 2)\n",
    "    \n",
    "    dhl3_v = dhl3_v * decay_rate_2 + (1- decay_rate_2) * (dhlw_3 ** 2)\n",
    "    dhlb3_v = dhlb3_v * decay_rate_2 + (1- decay_rate_2) * (dhlb_3 ** 2)\n",
    "    \n",
    "    dhl2_v = dhl2_v * decay_rate_2 + (1- decay_rate_2) * (dhlw_2 ** 2)\n",
    "    dhlb2_v = dhlb2_v * decay_rate_2 + (1- decay_rate_2) * (dhlb_2 ** 2)\n",
    "    \n",
    "    dhl1_v = dhl1_v * decay_rate_2 + (1- decay_rate_2) * (dhlw_1 ** 2)\n",
    "    dhlb1_v = dhlb1_v * decay_rate_2 + (1- decay_rate_2) * (dhlb_1 ** 2)\n",
    "            \n",
    "    dow_m_corrected  = dow_m/(1-(decay_rate_1 ** t))\n",
    "    dow_v_corrected  = dow_v/(1-(decay_rate_2 ** t))\n",
    "    dob_m_corrected  = dob_m/(1-(decay_rate_1 ** t))\n",
    "    dob_v_corrected  = dob_v/(1-(decay_rate_2 ** t))\n",
    "    \n",
    "    dhl3_m_corrected  = dhl3_m/(1-(decay_rate_1 ** t))\n",
    "    dhl3_v_corrected  = dhl3_v/(1-(decay_rate_2 ** t))\n",
    "    dhlb3_m_corrected  = dhlb3_m/(1-(decay_rate_1 ** t))\n",
    "    dhlb3_v_corrected  = dhlb3_v/(1-(decay_rate_2 ** t))\n",
    "\n",
    "    dhl2_m_corrected  = dhl2_m/(1-(decay_rate_1 ** t))\n",
    "    dhl2_v_corrected  = dhl2_v/(1-(decay_rate_2 ** t))\n",
    "    dhlb2_m_corrected  = dhlb2_m/(1-(decay_rate_1 ** t))\n",
    "    dhlb2_v_corrected  = dhlb2_v/(1-(decay_rate_2 ** t))\n",
    "\n",
    "    dhl1_m_corrected  = dhl1_m/(1-(decay_rate_1 ** t))\n",
    "    dhl1_v_corrected  = dhl1_v/(1-(decay_rate_2 ** t))\n",
    "    dhlb1_m_corrected  = dhlb1_m/(1-(decay_rate_1 ** t))\n",
    "    dhlb1_v_corrected  = dhlb1_v/(1-(decay_rate_2 ** t))\n",
    "              \n",
    "    # Update Weights\n",
    "    dow = dow_m_corrected / (np.sqrt(dow_v_corrected) + epsilon)\n",
    "    dob = dob_m_corrected / (np.sqrt(dob_v_corrected) + epsilon)\n",
    "    dhlw_3 = dhl3_m_corrected / (np.sqrt(dhl3_v_corrected) + epsilon)\n",
    "    dhlb_3 = dhlb3_m_corrected / (np.sqrt(dhlb3_v_corrected) + epsilon)\n",
    "    dhlw_2 = dhl2_m_corrected / (np.sqrt(dhl2_v_corrected) + epsilon)\n",
    "    dhlb_2 = dhlb2_m_corrected / (np.sqrt(dhlb2_v_corrected) + epsilon)\n",
    "    dhlw_1 = dhl1_m_corrected / (np.sqrt(dhl1_v_corrected) + epsilon)\n",
    "    dhlb_1 = dhlb1_m_corrected / (np.sqrt(dhlb1_v_corrected) + epsilon)\n",
    "            \n",
    "    hl1_weights -= alpha*dhlw_1\n",
    "    hl1_biases -= alpha*dhlb_1\n",
    "    \n",
    "    hl2_weights -= alpha*dhlw_2\n",
    "    hl2_biases -= alpha*dhlb_2\n",
    "    \n",
    "    hl3_weights -= alpha*dhlw_3\n",
    "    hl3_biases -= alpha*dhlb_3\n",
    "    \n",
    "    ol_weights -= alpha*dow\n",
    "    ol_biases -= alpha*dob\n",
    "    weights_and_biases = (hl1_weights,hl1_biases, hl2_weights,hl2_biases, hl3_weights,hl3_biases, ol_weights,ol_biases)\n",
    "    adam_parameters = (dow_m, dow_v,dob_m, dob_v, dhl3_m, dhl3_v, dhlb3_m, dhlb3_v, dhl2_m, dhl2_v, dhlb2_m, dhlb2_v, dhl1_m, dhl1_v, dhlb1_m, dhlb1_v, t)\n",
    "    \n",
    "    return (weights_and_biases, adam_parameters)\n",
    "\n",
    "#def train_network(input_data,input_label,weights_and_biases,alpha, adam_parameters):\n",
    "def train_network(input_data,input_label,weights_and_biases,alpha):    \n",
    "    #feedforward\n",
    "    initial_values=feedforward(input_data,weights_and_biases)   \n",
    "    \n",
    "    #backpropagation    \n",
    "    weights_and_biases=backpropogate(input_data,input_label,initial_values,weights_and_biases,alpha)    \n",
    "    #weights_and_biases, adam_parameters=backpropogate_adam(adam_parameters, input_data,input_label,initial_values,weights_and_biases,alpha,\n",
    "    #                                    decay_rate_1 = 0.9,\n",
    "    #                                      decay_rate_2 = 0.99,\n",
    "    #                                      epsilon = 10e-8)    \n",
    "    #return (weights_and_biases,initial_values[0], adam_parameters)\n",
    "    return (weights_and_biases,initial_values[0])\n",
    "\n",
    "def prediction(input_data,weights_and_biases):\n",
    "    return feedforward(input_data,weights_and_biases)[0]\n",
    "    \n",
    "def accuracy(data,labels,weights_and_biases,size_data):\n",
    "    total = 0\n",
    "    for i in range(size_data):\n",
    "        if(np.argmax(prediction(data[i],weights_and_biases)) == np.argmax(labels[i])):\n",
    "            total += 1\n",
    "    return ((100.0 * total)/size_data)\n",
    "\n",
    "def accuracy_end(data,labels,weights_and_biases,size_data):\n",
    "    total = 0\n",
    "    value = []\n",
    "    for i in range(size_data):\n",
    "        value.append(prediction(data[i],weights_and_biases))\n",
    "        if(np.argmax(value) == np.argmax(labels[i])):\n",
    "            total += 1\n",
    "    return (value)\n",
    "\n",
    "def residuals(data,labels,weights_and_biases,size_data):\n",
    "    total = 0\n",
    "    for i in range(size_data):\n",
    "        total+=((prediction(data[i],weights_and_biases) - labels[i])** 2.0).mean(axis=1)\n",
    "    return ((100.0 * total)/size_data)\n",
    "\n",
    "accuracy_list = []\n",
    "residuals_list = []\n",
    "\n",
    "dow_m = 0\n",
    "dow_v = 0\n",
    "dob_m = 0\n",
    "dob_v = 0\n",
    "dhl3_m = 0\n",
    "dhl3_v = 0\n",
    "dhlb3_m = 0\n",
    "dhlb3_v = 0\n",
    "dhl2_m = 0\n",
    "dhl2_v = 0\n",
    "dhlb2_m = 0\n",
    "dhlb2_v = 0\n",
    "dhl1_m = 0\n",
    "dhl1_v = 0\n",
    "dhlb1_m = 0\n",
    "dhlb1_v = 0\n",
    "t = 0\n",
    "\n",
    "adam_parameters = (dow_m, dow_v, dob_m, dob_v, dhl3_m, dhl3_v, dhlb3_m, dhlb3_v, dhl2_m, dhl2_v, dhlb2_m, \n",
    "                       dhlb2_v, dhl1_m, dhl1_v, dhlb1_m, dhlb1_v, t)\n",
    "for j in range(train_size):\n",
    "    for i in range(batch):\n",
    "        training = train_data[j].reshape((1,train_data[j].shape[0]))\n",
    "        #weights_and_biases,predict, adam_parameters = train_network(training,train_labels[j],weights_and_biases,learning_rate, adam_parameters)\n",
    "        weights_and_biases,predict = train_network(training,train_labels[j],weights_and_biases,learning_rate)\n",
    "        tr_label = train_labels[j].reshape((1,train_labels[j].shape[0]))\n",
    "        if (residuals(training,tr_label,weights_and_biases,1)==0.0):\n",
    "            break        \n",
    "\n",
    "    if j%1000==0:\n",
    "       \n",
    "        #print('\\nTraining accuracy for current:',accuracy(training,tr_label,weights_and_biases,1),\"%\")       \n",
    "        acc = accuracy(test_data,test_labels,weights_and_biases,test_size)\n",
    "        print('\\nTest accuracy:',acc,\"%\")\n",
    "        accuracy_list.append(acc)\n",
    "        #print('\\nTraining error:',residuals(training,tr_label,weights_and_biases,1))\n",
    "        res = residuals(test_data,test_labels,weights_and_biases,test_size)\n",
    "        print('\\nTest error:',res)\n",
    "        residuals_list.append(res)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.show()\n",
    "value = accuracy_end(test_data,test_labels,weights_and_biases,test_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
